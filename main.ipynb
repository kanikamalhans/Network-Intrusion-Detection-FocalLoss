{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "mimjrpoDPAqQ",
        "outputId": "f16ee66e-9565-4975-9823-ff2e79db4985"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-d5df0069828e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    135\u001b[0m   )\n\u001b[1;32m    136\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     if (\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/musimathicslab/network-intrusion-detection-gnn.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CUOdxFqDPUEU",
        "outputId": "74c7cf33-210f-44d8-bdd3-d468e93fe803"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'network-intrusion-detection-gnn'...\n",
            "remote: Enumerating objects: 61, done.\u001b[K\n",
            "remote: Counting objects: 100% (61/61), done.\u001b[K\n",
            "remote: Compressing objects: 100% (58/58), done.\u001b[K\n",
            "remote: Total 61 (delta 27), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (61/61), 23.41 KiB | 3.34 MiB/s, done.\n",
            "Resolving deltas: 100% (27/27), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd network-intrusion-detection-gnn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-OOsTh6XPYVC",
        "outputId": "8b10be28-b894-4e14-c9f5-5329ffc5a737"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: 'network-intrusion-detection-gnn'\n",
            "/content/network-intrusion-detection-gnn\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(os.listdir(\"dataset/raw\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YuVS2K-NQrej",
        "outputId": "e5117530-f496-4be8-891a-1c9c3a80f72e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['UNSW-NB15_1.csv', 'UNSW-NB15_4.csv', 'UNSW-NB15_3.csv', 'UNSW-NB15_2.csv']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# List all files in the directory\n",
        "file_list = os.listdir(\"dataset/raw\")\n",
        "\n",
        "# Select any one file, e.g., the first one\n",
        "file_path = os.path.join(\"dataset/raw\", file_list[1])\n",
        "\n",
        "# Load the CSV file into a DataFrame\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Print the feature names (column names)\n",
        "print(df.columns.tolist())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e-i_yyEF7nlf",
        "outputId": "5e41d78f-2fb5-4a47-938b-d1470d616091"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['59.166.0.9', '7045', '149.171.126.7', '25', 'tcp', 'FIN', '0.201886', '37552', '3380', '31', '29', '18', '8', 'smtp', '1459437.5', '130766.8672', '52', '42', '255', '255.1', '1422136554', '3572668484', '722', '80', '0', '0.1', '456.043567', '15.530109', '1424250009', '1424250009.1', '3.943843', '4.912488', '0.00059', '0.000473', '0.000117', '0.2', '0.3', 'Unnamed: 37', 'Unnamed: 38', ' ', '2', '2.1', '7', '4', '1', '1.1', '3', 'Unnamed: 47', '0.4']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "AWP-Hg3IQs6f",
        "outputId": "b4619e20-bb64-40c2-ce6f-63fe652110de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting matplotlib~=3.7.0 (from -r requirements.txt (line 1))\n",
            "  Downloading matplotlib-3.7.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.7 kB)\n",
            "Collecting seaborn~=0.12.2 (from -r requirements.txt (line 2))\n",
            "  Downloading seaborn-0.12.2-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting pandas~=1.5.3 (from -r requirements.txt (line 3))\n",
            "  Downloading pandas-1.5.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Collecting numpy~=1.24.2 (from -r requirements.txt (line 4))\n",
            "  Downloading numpy-1.24.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: networkx~=3.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 5)) (3.4.2)\n",
            "Collecting torch~=1.13.1 (from -r requirements.txt (line 6))\n",
            "  Downloading torch-1.13.1-cp311-cp311-manylinux1_x86_64.whl.metadata (24 kB)\n",
            "Collecting tqdm~=4.64.1 (from -r requirements.txt (line 7))\n",
            "  Downloading tqdm-4.64.1-py2.py3-none-any.whl.metadata (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.3/57.3 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting imblearn~=0.0 (from -r requirements.txt (line 8))\n",
            "  Downloading imblearn-0.0-py2.py3-none-any.whl.metadata (355 bytes)\n",
            "Collecting scikit-learn~=1.2.1 (from -r requirements.txt (line 9))\n",
            "  Downloading scikit_learn-1.2.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Collecting xgboost~=1.7.4 (from -r requirements.txt (line 10))\n",
            "  Downloading xgboost-1.7.6-py3-none-manylinux2014_x86_64.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib~=3.7.0->-r requirements.txt (line 1)) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib~=3.7.0->-r requirements.txt (line 1)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib~=3.7.0->-r requirements.txt (line 1)) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib~=3.7.0->-r requirements.txt (line 1)) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib~=3.7.0->-r requirements.txt (line 1)) (24.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib~=3.7.0->-r requirements.txt (line 1)) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib~=3.7.0->-r requirements.txt (line 1)) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib~=3.7.0->-r requirements.txt (line 1)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas~=1.5.3->-r requirements.txt (line 3)) (2025.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from torch~=1.13.1->-r requirements.txt (line 6)) (4.12.2)\n",
            "Collecting nvidia-cuda-runtime-cu11==11.7.99 (from torch~=1.13.1->-r requirements.txt (line 6))\n",
            "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu11==8.5.0.96 (from torch~=1.13.1->-r requirements.txt (line 6))\n",
            "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu11==11.10.3.66 (from torch~=1.13.1->-r requirements.txt (line 6))\n",
            "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch~=1.13.1->-r requirements.txt (line 6))\n",
            "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch~=1.13.1->-r requirements.txt (line 6)) (75.1.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.11/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch~=1.13.1->-r requirements.txt (line 6)) (0.45.1)\n",
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.11/dist-packages (from imblearn~=0.0->-r requirements.txt (line 8)) (0.13.0)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from scikit-learn~=1.2.1->-r requirements.txt (line 9)) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn~=1.2.1->-r requirements.txt (line 9)) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn~=1.2.1->-r requirements.txt (line 9)) (3.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib~=3.7.0->-r requirements.txt (line 1)) (1.17.0)\n",
            "INFO: pip is looking at multiple versions of imbalanced-learn to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting imbalanced-learn (from imblearn~=0.0->-r requirements.txt (line 8))\n",
            "  Downloading imbalanced_learn-0.12.4-py3-none-any.whl.metadata (8.3 kB)\n",
            "Downloading matplotlib-3.7.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m64.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading seaborn-0.12.2-py3-none-any.whl (293 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m293.3/293.3 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pandas-1.5.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m55.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.24.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m38.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torch-1.13.1-cp311-cp311-manylinux1_x86_64.whl (887.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m887.4/887.4 MB\u001b[0m \u001b[31m858.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m78.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tqdm-4.64.1-py2.py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading imblearn-0.0-py2.py3-none-any.whl (1.9 kB)\n",
            "Downloading scikit_learn-1.2.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m75.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xgboost-1.7.6-py3-none-manylinux2014_x86_64.whl (200.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.3/200.3 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading imbalanced_learn-0.12.4-py3-none-any.whl (258 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m258.3/258.3 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tqdm, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cublas-cu11, numpy, pandas, nvidia-cudnn-cu11, xgboost, torch, scikit-learn, matplotlib, seaborn, imbalanced-learn, imblearn\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.67.1\n",
            "    Uninstalling tqdm-4.67.1:\n",
            "      Successfully uninstalled tqdm-4.67.1\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.4\n",
            "    Uninstalling numpy-1.26.4:\n",
            "      Successfully uninstalled numpy-1.26.4\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.2.2\n",
            "    Uninstalling pandas-2.2.2:\n",
            "      Successfully uninstalled pandas-2.2.2\n",
            "  Attempting uninstall: xgboost\n",
            "    Found existing installation: xgboost 2.1.4\n",
            "    Uninstalling xgboost-2.1.4:\n",
            "      Successfully uninstalled xgboost-2.1.4\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.5.1+cu124\n",
            "    Uninstalling torch-2.5.1+cu124:\n",
            "      Successfully uninstalled torch-2.5.1+cu124\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.6.1\n",
            "    Uninstalling scikit-learn-1.6.1:\n",
            "      Successfully uninstalled scikit-learn-1.6.1\n",
            "  Attempting uninstall: matplotlib\n",
            "    Found existing installation: matplotlib 3.10.0\n",
            "    Uninstalling matplotlib-3.10.0:\n",
            "      Successfully uninstalled matplotlib-3.10.0\n",
            "  Attempting uninstall: seaborn\n",
            "    Found existing installation: seaborn 0.13.2\n",
            "    Uninstalling seaborn-0.13.2:\n",
            "      Successfully uninstalled seaborn-0.13.2\n",
            "  Attempting uninstall: imbalanced-learn\n",
            "    Found existing installation: imbalanced-learn 0.13.0\n",
            "    Uninstalling imbalanced-learn-0.13.0:\n",
            "      Successfully uninstalled imbalanced-learn-0.13.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 1.5.3 which is incompatible.\n",
            "mlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\n",
            "plotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.5 which is incompatible.\n",
            "plotnine 0.14.5 requires pandas>=2.2.0, but you have pandas 1.5.3 which is incompatible.\n",
            "pymc 5.20.1 requires numpy>=1.25.0, but you have numpy 1.24.4 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 1.24.4 which is incompatible.\n",
            "torchaudio 2.5.1+cu124 requires torch==2.5.1, but you have torch 1.13.1 which is incompatible.\n",
            "mizani 0.13.1 requires pandas>=2.2.0, but you have pandas 1.5.3 which is incompatible.\n",
            "torchvision 0.20.1+cu124 requires torch==2.5.1, but you have torch 1.13.1 which is incompatible.\n",
            "xarray 2025.1.2 requires pandas>=2.1, but you have pandas 1.5.3 which is incompatible.\n",
            "accelerate 1.3.0 requires torch>=2.0.0, but you have torch 1.13.1 which is incompatible.\n",
            "treescope 0.1.8 requires numpy>=1.25.2, but you have numpy 1.24.4 which is incompatible.\n",
            "cudf-cu12 24.12.0 requires pandas<2.2.4dev0,>=2.0, but you have pandas 1.5.3 which is incompatible.\n",
            "langchain 0.3.18 requires numpy<2,>=1.26.4; python_version < \"3.12\", but you have numpy 1.24.4 which is incompatible.\n",
            "blosc2 3.0.0 requires numpy>=1.25.0, but you have numpy 1.24.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed imbalanced-learn-0.12.4 imblearn-0.0 matplotlib-3.7.5 numpy-1.24.4 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 pandas-1.5.3 scikit-learn-1.2.2 seaborn-0.12.2 torch-1.13.1 tqdm-4.64.1 xgboost-1.7.6\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "matplotlib",
                  "mpl_toolkits",
                  "numpy",
                  "pandas"
                ]
              },
              "id": "d09407f54ecd48248512c8f990efc817"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch-geometric"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yrzWQW6LRigW",
        "outputId": "26b1f5fb-a6be-479b-ab65-d6bf4865f400"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch-geometric\n",
            "  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/63.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.11.12)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2024.10.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.1.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (1.24.4)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.2.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (4.64.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.18.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch-geometric) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2025.1.31)\n",
            "Downloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch-geometric\n",
            "Successfully installed torch-geometric-2.6.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/network-intrusion-detection-gnn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MfHjTZnoRx9V",
        "outputId": "3a150e62-c2f0-4413-921e-496a3d0c31f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/network-intrusion-detection-gnn\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sed -i \"s/mpl.use('TkAgg')/mpl.use('Agg')/\" /content/network-intrusion-detection-gnn/utils/util.py\n"
      ],
      "metadata": {
        "id": "gmgpDHLNS37o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python nb15_pre_processing.py"
      ],
      "metadata": {
        "id": "j34B0zEES9AT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python nb15_main.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cvxtUX01Ydhg",
        "outputId": "3fad45e0-e1f5-465b-fdf7-80766e88bbad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing...\n",
            "Counter({0: 1331415, 9: 129298, 7: 129298, 4: 129298, 8: 129298, 6: 129298, 5: 129298, 2: 129298, 3: 129298, 1: 129298})\n",
            "Creating nodes...: 100% 2495097/2495097 [02:10<00:00, 19075.22it/s]\n",
            "Creating edges for features: ['proto', 'service', 'state']: 100% 184/184 [00:15<00:00, 11.92it/s]\n",
            "Done!\n",
            "Processing...\n",
            "Counter({0: 443831, 9: 43110, 8: 8732, 7: 4784, 6: 3315, 5: 2850, 4: 550, 2: 477, 3: 320, 1: 41})\n",
            "Creating nodes...: 100% 508010/508010 [00:25<00:00, 19576.20it/s]\n",
            "Creating edges for features: ['proto', 'service', 'state']: 100% 177/177 [00:04<00:00, 36.46it/s]\n",
            "Done!\n",
            "Processing...\n",
            "Counter({0: 443518, 9: 43073, 8: 8997, 7: 5032, 6: 3304, 5: 2784, 4: 545, 2: 441, 3: 278, 1: 38})\n",
            "Creating nodes...: 100% 508010/508010 [00:25<00:00, 19644.86it/s]\n",
            "Creating edges for features: ['proto', 'service', 'state']: 100% 175/175 [00:04<00:00, 36.57it/s]\n",
            "Done!\n",
            "Number of features: 26\n",
            "Number of classes: 10\n",
            "\n",
            "------------------------------------\n",
            "\n",
            "Data(edge_index=[2, 3047686], y=[2495097], x=[2495097, 26])\n",
            "Data(edge_index=[2, 1015666], y=[508010], x=[508010, 26])\n",
            "Data(edge_index=[2, 1015670], y=[508010], x=[508010, 26])\n",
            "\n",
            "------------------------------------\n",
            "\n",
            "N. Convs: 64\n",
            "N. Hidden Channels: 512\n",
            "\n",
            "------------------------------------\n",
            "\n",
            "N. Convs: 64\n",
            "N. Hidden Channels: 512\n",
            "\n",
            "------------------------------------\n",
            "\n",
            "NodeClassificator(\n",
            "  (lin1): Linear(in_features=26, out_features=128, bias=True)\n",
            "  (convs): ModuleList(\n",
            "    (0): GCN2Conv(128, alpha=0.5, beta=0.5306282510621704)\n",
            "    (1): GCN2Conv(128, alpha=0.5, beta=0.30010459245033816)\n",
            "    (2): GCN2Conv(128, alpha=0.5, beta=0.2097205309820691)\n",
            "    (3): GCN2Conv(128, alpha=0.5, beta=0.16126814759612232)\n",
            "    (4): GCN2Conv(128, alpha=0.5, beta=0.131028262406404)\n",
            "    (5): GCN2Conv(128, alpha=0.5, beta=0.11034805716886541)\n",
            "    (6): GCN2Conv(128, alpha=0.5, beta=0.09531017980432493)\n",
            "    (7): GCN2Conv(128, alpha=0.5, beta=0.08388148398070203)\n",
            "  )\n",
            "  (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            "  (fc1): Linear(in_features=128, out_features=128, bias=True)\n",
            "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
            ")\n",
            "\n",
            "------------------------------------\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
            "----------------------------------------------------------------------\n",
            "   1    |   255   |   1.027752   |     -      |     -     |  156.14  \n",
            "----------------------------------------------------------------------\n",
            "   1    |    -    |   1.027752   |  1.106482  |   94.80   |  171.42  \n",
            "----------------------------------------------------------------------\n",
            "   2    |   255   |   0.889899   |     -      |     -     |  155.53  \n",
            "----------------------------------------------------------------------\n",
            "   2    |    -    |   0.889899   |  1.054043  |   95.22   |  170.74  \n",
            "----------------------------------------------------------------------\n",
            "   3    |   255   |   0.852562   |     -      |     -     |  157.40  \n",
            "----------------------------------------------------------------------\n",
            "   3    |    -    |   0.852562   |  1.030207  |   95.55   |  172.50  \n",
            "----------------------------------------------------------------------\n",
            "   4    |   255   |   0.827917   |     -      |     -     |  153.94  \n",
            "----------------------------------------------------------------------\n",
            "   4    |    -    |   0.827917   |  1.008473  |   95.66   |  169.08  \n",
            "----------------------------------------------------------------------\n",
            "   5    |   255   |   0.805755   |     -      |     -     |  153.12  \n",
            "----------------------------------------------------------------------\n",
            "   5    |    -    |   0.805755   |  1.024970  |   95.75   |  168.31  \n",
            "----------------------------------------------------------------------\n",
            "   6    |   255   |   0.790338   |     -      |     -     |  154.48  \n",
            "----------------------------------------------------------------------\n",
            "   6    |    -    |   0.790338   |  0.952448  |   96.16   |  170.16  \n",
            "----------------------------------------------------------------------\n",
            "   7    |   255   |   0.776228   |     -      |     -     |  152.09  \n",
            "----------------------------------------------------------------------\n",
            "   7    |    -    |   0.776228   |  0.955067  |   96.21   |  167.15  \n",
            "----------------------------------------------------------------------\n",
            "   8    |   255   |   0.764222   |     -      |     -     |  152.58  \n",
            "----------------------------------------------------------------------\n",
            "   8    |    -    |   0.764222   |  0.959643  |   96.25   |  167.71  \n",
            "----------------------------------------------------------------------\n",
            "   9    |   255   |   0.757174   |     -      |     -     |  151.95  \n",
            "----------------------------------------------------------------------\n",
            "   9    |    -    |   0.757174   |  0.928523  |   96.22   |  167.00  \n",
            "----------------------------------------------------------------------\n",
            "  10    |   255   |   0.749384   |     -      |     -     |  151.86  \n",
            "----------------------------------------------------------------------\n",
            "  10    |    -    |   0.749384   |  0.933148  |   96.42   |  166.95  \n",
            "----------------------------------------------------------------------\n",
            "  11    |   255   |   0.742908   |     -      |     -     |  151.79  \n",
            "----------------------------------------------------------------------\n",
            "  11    |    -    |   0.742908   |  0.960147  |   96.22   |  167.41  \n",
            "----------------------------------------------------------------------\n",
            "  12    |   255   |   0.739107   |     -      |     -     |  150.96  \n",
            "----------------------------------------------------------------------\n",
            "  12    |    -    |   0.739107   |  0.962570  |   96.22   |  166.77  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "Training complete!\n",
            "\n",
            "------------------------------------\n",
            "\n",
            "Accuracy on test: 0.9618688608491959\n",
            "Balanced accuracy on test: 0.6127905849332888\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      1.000     0.982     0.991    443518\n",
            "           1      0.043     0.737     0.082        38\n",
            "           2      0.051     0.057     0.054       441\n",
            "           3      0.070     0.777     0.129       278\n",
            "           4      0.096     0.413     0.156       545\n",
            "           5      0.469     0.362     0.409      2784\n",
            "           6      0.311     0.689     0.429      3304\n",
            "           7      0.394     0.686     0.501      5032\n",
            "           8      0.690     0.452     0.546      8997\n",
            "           9      1.000     0.973     0.986     43073\n",
            "\n",
            "    accuracy                          0.962    508010\n",
            "   macro avg      0.413     0.613     0.428    508010\n",
            "weighted avg      0.979     0.962     0.969    508010\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python nb15_ml_main.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TFZgP5dG70mU",
        "outputId": "b5f4fd52-c793-45f2-9c11-99a9136b0984"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on test: 0.993309186827031\n",
            "Balanced accuracy on test: 0.9829037871364338\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.995     0.997     0.996    443518\n",
            "           1      0.978     0.969     0.974     64492\n",
            "\n",
            "    accuracy                          0.993    508010\n",
            "   macro avg      0.987     0.983     0.985    508010\n",
            "weighted avg      0.993     0.993     0.993    508010\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#model.py\n",
        "import numpy as np\n",
        "import time\n",
        "import os\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import LayerNorm\n",
        "from torch_geometric.nn.conv import GCN2Conv\n",
        "\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, weight=None, gamma=2):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.weight = weight  # Class weights (for imbalance)\n",
        "        self.gamma = gamma    # Focus parameter for hard examples\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        ce_loss = F.cross_entropy(inputs, targets, reduction='none', weight=self.weight)\n",
        "        pt = torch.exp(-ce_loss)\n",
        "        focal_loss = (1 - pt) ** self.gamma * ce_loss\n",
        "        return focal_loss.mean()\n",
        "\n",
        "class NodeClassificator(nn.Module):\n",
        "    def __init__(self, dataset, num_classes, num_convs: int = 8, hid: int = 128, dropout: float = 0.5,\n",
        "                 alpha: float = 0.5, theta: float = 0.7):\n",
        "        super(NodeClassificator, self).__init__()\n",
        "        hid=128\n",
        "        num_convs=8\n",
        "        self.hid = hid\n",
        "        self.num_layers = num_convs\n",
        "\n",
        "        self.lin1 = nn.Linear(dataset.num_node_features, hid)\n",
        "\n",
        "        self.convs = nn.ModuleList()\n",
        "        for layer_index in range(num_convs):\n",
        "            conv = GCN2Conv(\n",
        "                channels=hid,\n",
        "                alpha=alpha,\n",
        "                theta=theta,\n",
        "                layer=layer_index + 1,\n",
        "                normalize=True\n",
        "            )\n",
        "            self.convs.append(conv)\n",
        "\n",
        "        self.norm = LayerNorm(hid, elementwise_affine=True)\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        self.fc1 = nn.Linear(hid, hid)\n",
        "        self.fc2 = nn.Linear(hid, num_classes)\n",
        "\n",
        "    def forward(self, x, edge_index, edge_attr):\n",
        "        x = self.lin1(x)\n",
        "        x0 = x\n",
        "\n",
        "        for conv in self.convs:\n",
        "            x = self.dropout(x)\n",
        "            x = F.gelu(conv(x, x0, edge_index))\n",
        "\n",
        "        x = F.gelu(self.norm(x))\n",
        "        x = F.dropout(x, p=0.5, training=self.training)\n",
        "\n",
        "        x = F.gelu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        # x = F.softmax(x, dim=1)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "def train(model,\n",
        "          train_dataloader,\n",
        "          criterion,\n",
        "          criterion_val,\n",
        "          optimizer,\n",
        "          device,\n",
        "          model_path,\n",
        "          logger=None,\n",
        "          epochs=12,\n",
        "          model_name='gnn',\n",
        "          evaluation=False,\n",
        "          patience=10,\n",
        "          val_dataloader=None):\n",
        "    # Print the header of the result table\n",
        "    logger.info(f\"{'Epoch':^7} | {'Batch':^7} | {'Train Loss':^12} | {'Val Loss':^10} | {'Val Acc':^9} | {'Elapsed':^9}\")\n",
        "    logger.info(\"-\" * 70)\n",
        "    trigger_times = 0\n",
        "    last_loss = np.inf\n",
        "    best_model = None\n",
        "    for epoch_i in range(epochs):\n",
        "        # Measure the elapsed time of each epoch\n",
        "        t0_epoch, t0_batch = time.time(), time.time()\n",
        "        # Reset tracking variables at the beginning of each epoch\n",
        "        total_loss, batch_loss, batch_counts = 0, 0, 0\n",
        "        # Put the model into the training mode [IT'S JUST A FLAG]\n",
        "        model.train()\n",
        "\n",
        "        # For each batch of training data...\n",
        "        for step, batch in enumerate(train_dataloader):\n",
        "            batch_counts += 1\n",
        "            # Zero out any previously calculated gradients\n",
        "            optimizer.zero_grad()\n",
        "            # Load batch to GPU\n",
        "            batch = batch.to(device)\n",
        "\n",
        "            # Compute loss and accumulate the loss values\n",
        "            output = model(batch.x, batch.edge_index, batch.edge_attr)\n",
        "            target = batch.y\n",
        "            loss = criterion(output, target)\n",
        "            batch_loss += loss.item()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Perform a backward pass to calculate gradients\n",
        "            loss.backward()\n",
        "            # Update parameters and the learning rate\n",
        "            optimizer.step()\n",
        "\n",
        "            # Print the loss values and time elapsed for every 20 batches\n",
        "            if (step % 500 == 0 and step != 0) or (step == len(train_dataloader) - 1):\n",
        "                # Calculate time elapsed for 20 batches\n",
        "                time_elapsed = time.time() - t0_batch\n",
        "\n",
        "                # Print training results\n",
        "                logger.info(\n",
        "                    f\"{epoch_i + 1:^7} | {step:^7} | {batch_loss / batch_counts:^12.6f} | {'-':^10} | {'-':^9} | \"\n",
        "                    f\"{time_elapsed:^9.2f}\")\n",
        "\n",
        "                # Reset batch tracking variables\n",
        "                batch_loss, batch_counts = 0, 0\n",
        "                t0_batch = time.time()\n",
        "\n",
        "        # Calculate the average loss over the entire training data\n",
        "        avg_train_loss = total_loss / len(train_dataloader)\n",
        "\n",
        "        logger.info(\"-\" * 70)\n",
        "\n",
        "        if evaluation:\n",
        "            # After the completion of each training epoch, measure the model's performance\n",
        "            # on our validation set.\n",
        "            val_loss, val_accuracy = evaluate(model, val_dataloader, criterion_val, device)\n",
        "\n",
        "            # Print performance over the entire training data\n",
        "            time_elapsed = time.time() - t0_epoch\n",
        "\n",
        "            logger.info(\n",
        "                f\"{epoch_i + 1:^7} | {'-':^7} | {avg_train_loss:^12.6f} | {val_loss:^10.6f} | {val_accuracy:^9.2f} |\"\n",
        "                f\" {time_elapsed:^9.2f}\")\n",
        "            logger.info(\"-\" * 70)\n",
        "\n",
        "            # Early stopping\n",
        "            if val_loss > last_loss:\n",
        "                trigger_times += 1\n",
        "            else:\n",
        "                last_loss = val_loss\n",
        "                best_model = model.state_dict()\n",
        "                trigger_times = 0\n",
        "\n",
        "            if trigger_times >= patience:\n",
        "                torch.save(best_model, os.path.join(model_path, f'{model_name}.h5'))\n",
        "                break\n",
        "\n",
        "        else:\n",
        "            # Early stopping\n",
        "            if avg_train_loss > last_loss:\n",
        "                trigger_times += 1\n",
        "            else:\n",
        "                last_loss = avg_train_loss\n",
        "                best_model = model.state_dict()\n",
        "                trigger_times = 0\n",
        "\n",
        "            if trigger_times >= patience:\n",
        "                torch.save(best_model, os.path.join(model_path, f'{model_name}.h5'))\n",
        "                break\n",
        "\n",
        "    torch.save(model.state_dict() if best_model is None else best_model, os.path.join(model_path, f'{model_name}.h5'))\n",
        "    logger.info(\"\\nTraining complete!\")\n",
        "\n",
        "\n",
        "def evaluate(model,\n",
        "             val_dataloader,\n",
        "             criterion,\n",
        "             device):\n",
        "    # Put the model into the evaluation mode. The dropout layers are disabled during the test time.\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables\n",
        "    val_accuracy = []\n",
        "    val_loss = []\n",
        "\n",
        "    # For each batch in our validation set...\n",
        "    for batch in val_dataloader:\n",
        "        # Load batch to GPU\n",
        "        batch = batch.to(device)\n",
        "\n",
        "        # Compute logits\n",
        "        with torch.no_grad():\n",
        "            output = model(batch.x, batch.edge_index, batch.edge_attr)\n",
        "\n",
        "            # Compute loss\n",
        "            loss = criterion(output, batch.y)\n",
        "            val_loss.append(loss.item())\n",
        "\n",
        "            # Get the predictions\n",
        "            probs = F.softmax(output, dim=1)\n",
        "            preds = torch.argmax(probs, dim=1)\n",
        "            # Calculate the accuracy rate\n",
        "            accuracy = (preds == batch.y).cpu().numpy().mean() * 100\n",
        "\n",
        "            val_accuracy.append(accuracy)\n",
        "\n",
        "    # Compute the average accuracy and loss over the validation set.\n",
        "    val_loss = np.mean(val_loss)\n",
        "    val_accuracy = np.mean(val_accuracy)\n",
        "\n",
        "    return val_loss, val_accuracy\n",
        "\n",
        "\n",
        "def predict(model,\n",
        "            test_dataloader,\n",
        "            device):\n",
        "    # Put the model into the evaluation mode. The dropout layers are disabled during the test time.\n",
        "    model.eval()\n",
        "    # Init outputs\n",
        "    outputs = []\n",
        "    y_true = []\n",
        "\n",
        "    # For each batch in our validation set...\n",
        "    for batch in test_dataloader:\n",
        "        #  Load batch to GPU\n",
        "        batch = batch.to(device)\n",
        "        # Compute logits\n",
        "        with torch.no_grad():\n",
        "            output = model(batch.x, batch.edge_index, batch.edge_attr)\n",
        "        y_true.append(batch.y)\n",
        "        outputs.append(output)\n",
        "\n",
        "\n",
        "    # Concatenate logits from each batch\n",
        "    outputs = torch.cat(outputs, dim=0)\n",
        "    y_true = torch.cat(y_true, dim=0)\n",
        "\n",
        "    # Apply softmax to calculate probabilities\n",
        "    probs = F.softmax(outputs, dim=1).cpu().numpy()\n",
        "    y_pred = np.argmax(probs, axis=1)\n",
        "    y_true = y_true.cpu().numpy()\n",
        "\n",
        "    return y_true, y_pred\n"
      ],
      "metadata": {
        "id": "C4wnuWJ873cu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#main.py\n",
        "from typing import final\n",
        "import numpy as np\n",
        "import argparse\n",
        "import os\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.optim import Adadelta\n",
        "from sklearn.utils import class_weight\n",
        "\n",
        "from nb15_pre_processing import MAPPING\n",
        "from utils.dataset import UNSWNB15NodeClassificationDataset\n",
        "from torch_geometric.loader import RandomNodeLoader\n",
        "\n",
        "# ========== IMPORT FOCAL LOSS ==========\n",
        "from utils.model import NodeClassificator, train, predict, FocalLoss\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import balanced_accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "from utils import SEPARATOR\n",
        "from utils import str2bool\n",
        "from utils import get_path_of_all\n",
        "from utils.util import create_directory\n",
        "from utils.util import plot_confusion_matrix\n",
        "from utils.util import plot_recall\n",
        "from utils.util import setup_logger\n",
        "\n",
        "NAME_DIR: final = '01 - UNSW-NB15'\n",
        "\n",
        "def node_classification(dataset_path: str, binary, n_neigh, augmentation, hid, num_convs):\n",
        "    # Get path of all file\n",
        "    log_path, log_file_path, log_train_path, model_path, result_path, \\\n",
        "        confusion_matrix_path, detection_rate_path = get_path_of_all(\n",
        "            NAME_DIR, num_neigh=n_neigh, hid=hid, n_convs=num_convs, augmentation=augmentation)\n",
        "\n",
        "    for directory in [log_path, model_path, confusion_matrix_path, detection_rate_path]:\n",
        "        create_directory(directory)\n",
        "\n",
        "    # Init logger\n",
        "    logger = setup_logger('logger', log_file_path)\n",
        "    train_logger = setup_logger('training', log_train_path)\n",
        "\n",
        "    # Create path of training and test dataset\n",
        "    _file_name_training = f'UNSW-NB15-train{\"-binary\" if binary else \"\"}.csv'\n",
        "    _file_path_training = os.path.join(dataset_path, 'raw', _file_name_training)\n",
        "    _file_name_val = f'UNSW-NB15-val{\"-binary\" if binary else \"\"}.csv'\n",
        "    _file_name_testing = f'UNSW-NB15-test{\"-binary\" if binary else \"\"}.csv'\n",
        "\n",
        "    # Create training, val and test dataset\n",
        "    train_dataset = UNSWNB15NodeClassificationDataset(\n",
        "        root=dataset_path,\n",
        "        file_name=_file_name_training,\n",
        "        binary=binary,\n",
        "        num_neighbors=n_neigh,\n",
        "        augmentation=augmentation,\n",
        "        val=False,\n",
        "        test=False\n",
        "    )\n",
        "    val_dataset = UNSWNB15NodeClassificationDataset(\n",
        "        root=dataset_path,\n",
        "        file_name=_file_name_val,\n",
        "        binary=binary,\n",
        "        num_neighbors=n_neigh,\n",
        "        val=True,\n",
        "        test=False\n",
        "    )\n",
        "    test_dataset = UNSWNB15NodeClassificationDataset(\n",
        "        root=dataset_path,\n",
        "        file_name=_file_name_testing,\n",
        "        binary=binary,\n",
        "        num_neighbors=n_neigh,\n",
        "        val=False,\n",
        "        test=True\n",
        "    )\n",
        "\n",
        "    logger.info(f'Number of features: {train_dataset.num_features}')\n",
        "    logger.info(f'Number of classes: {2 if binary else 10}')\n",
        "    logger.info(SEPARATOR)\n",
        "\n",
        "    # Define train, val and test loader\n",
        "    train_data = train_dataset[0]\n",
        "    val_data = val_dataset[0]\n",
        "    test_data = test_dataset[0]\n",
        "\n",
        "    # Log info of train, val and test\n",
        "    logger.info(train_data)\n",
        "    logger.info(val_data)\n",
        "    logger.info(test_data)\n",
        "    logger.info(SEPARATOR)\n",
        "\n",
        "    # Define train, val and test loader\n",
        "    train_loader = RandomNodeLoader(train_data, num_parts=256, shuffle=True)\n",
        "    val_loader = RandomNodeLoader(val_data, num_parts=256, shuffle=True)\n",
        "    test_loader = RandomNodeLoader(test_data, num_parts=256, shuffle=True)\n",
        "\n",
        "    # Define model\n",
        "    logger.info(f'N. Convs: {num_convs}')\n",
        "    logger.info(f'N. Hidden Channels: {hid}')\n",
        "    logger.info(SEPARATOR)\n",
        "    train_logger.info(f'N. Convs: {num_convs}')\n",
        "    train_logger.info(f'N. Hidden Channels: {hid}')\n",
        "    train_logger.info(SEPARATOR)\n",
        "    model = NodeClassificator(\n",
        "        dataset=train_data,\n",
        "        num_classes=2 if binary else 10,\n",
        "        num_convs=num_convs,\n",
        "        hid=hid,\n",
        "        alpha=0.5,\n",
        "        theta=0.7,\n",
        "        dropout=0.5\n",
        "    )\n",
        "\n",
        "    # Use GPU\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model.to(device)\n",
        "    logger.info(model)\n",
        "    logger.info(SEPARATOR)\n",
        "\n",
        "    # Define loss function and optimizer\n",
        "    y = train_dataset[0].y.numpy()\n",
        "    class_weights = class_weight.compute_class_weight('balanced', classes=np.unique(y), y=y)\n",
        "    class_weights = torch.tensor(class_weights, dtype=torch.float)\n",
        "\n",
        "    # ========== REPLACE TRAINING LOSS WITH FOCAL LOSS ==========\n",
        "    criterion = FocalLoss(weight=class_weights.to(device), gamma=2)  # Changed to FocalLoss\n",
        "    y_val = val_dataset[0].y.numpy()\n",
        "    class_weights_val = class_weight.compute_class_weight('balanced', classes=np.unique(y_val), y=y_val)\n",
        "    class_weights_val = torch.tensor(class_weights_val, dtype=torch.float)\n",
        "    criterion_val = nn.CrossEntropyLoss(weight=class_weights_val.to(device))  # Keep CE for validation\n",
        "\n",
        "    optimizer = Adadelta(model.parameters())\n",
        "\n",
        "    # Check if model is trained\n",
        "    model_name = f'gc_model_test_attack_{n_neigh}_hid_{hid}_convs_{num_convs}{\"_aug\" if augmentation else \"\"}'\n",
        "    model_trained_path = os.path.join(model_path, f'{model_name}.h5')\n",
        "    if os.path.exists(model_trained_path):\n",
        "        model.load_state_dict(torch.load(model_trained_path))\n",
        "    # Train model if is not trained\n",
        "    else:\n",
        "        train(\n",
        "            model,\n",
        "            train_loader,\n",
        "            criterion,\n",
        "            criterion_val,\n",
        "            optimizer,\n",
        "            device,\n",
        "            model_path,\n",
        "            logger=train_logger,\n",
        "            epochs=50,\n",
        "            model_name=model_name,\n",
        "            evaluation=True,\n",
        "            val_dataloader=val_loader,\n",
        "            patience=20\n",
        "        )\n",
        "        train_logger.info(SEPARATOR)\n",
        "\n",
        "    # Test model\n",
        "    y_true, y_pred = predict(\n",
        "        model,\n",
        "        test_loader,\n",
        "        device\n",
        "    )\n",
        "\n",
        "    logger.info(f'Accuracy on test: {accuracy_score(y_true, y_pred)}')\n",
        "    logger.info(f'Balanced accuracy on test: {balanced_accuracy_score(y_true, y_pred)}')\n",
        "    logger.info(f'\\n{classification_report(y_true, y_pred, digits=3)}')\n",
        "\n",
        "    # Plot results\n",
        "    if not binary:\n",
        "        image_name = f'node-classification_{n_neigh}' \\\n",
        "                     f'_hid_{hid}_convs_{num_convs}' \\\n",
        "                     f'{\"_aug\" if augmentation else \"\"}' \\\n",
        "                     f'.png'\n",
        "        # Confusion Matrix\n",
        "        cm = confusion_matrix(y_true, y_pred, normalize=\"true\")\n",
        "        plot_confusion_matrix(cm, MAPPING.keys(), os.path.join(confusion_matrix_path, image_name))\n",
        "        # Detection rate\n",
        "        recall = recall_score(y_true, y_pred, average=None)\n",
        "        plot_recall(MAPPING.keys(), recall, os.path.join(detection_rate_path, image_name))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser()\n",
        "\n",
        "    parser.add_argument('-b', dest='b', action='store',\n",
        "                        type=str2bool, default=False, help='true if you want binary classification')\n",
        "    parser.add_argument('-aug', dest='aug', action='store',\n",
        "                        type=str2bool, default=True, help='apply or not augmentation on data')\n",
        "    parser.add_argument('-neigh', dest='neigh', action='store',\n",
        "                        type=int, default=1, help='define number of neighborhood')\n",
        "    parser.add_argument('-hid', dest='hid', action='store',\n",
        "                        type=int, default=512, help='define number of hidden channels')\n",
        "    parser.add_argument('-n_convs', dest='n_convs', action='store',\n",
        "                        type=int, default=64, help='define number of convolution blocks')\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    node_classification(os.path.join(os.getcwd(), 'dataset'), args.b, args.neigh, args.aug, args.hid, args.n_convs)"
      ],
      "metadata": {
        "id": "q8yH4bnT76Zf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}